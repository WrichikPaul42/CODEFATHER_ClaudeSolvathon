{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anthropic\n",
    "import json\n",
    "\n",
    "# Initialize the client (reads ANTHROPIC_API_KEY from environment)\n",
    "try:\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Generate a single synthetic WIMP event as a JSON object.\n",
    "    The event should have keys: 's1_photons', 's2_charge', and 'recoil_energy_keV'.\n",
    "    Ensure the values are physically plausible for a low-energy nuclear recoil in a liquid xenon detector.\n",
    "    For example, s1 should be small (typically 5-50 photons for low-energy nuclear recoils), \n",
    "    and s2 should be relatively low for that s1 (roughly 100-2000 electrons).\n",
    "    Recoil energy should typically be in the range of 1-50 keV for WIMP searches.\n",
    "    \n",
    "    CRITICAL: Respond with ONLY a valid JSON object. No markdown, no code blocks, no extra text.\n",
    "    Format: {\"s1_photons\": <number>, \"s2_charge\": <number>, \"recoil_energy_keV\": <number>}\n",
    "    \"\"\"\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=256,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract response text\n",
    "    response_text = message.content[0].text.strip()\n",
    "    print(\"Raw Response from Claude:\")\n",
    "    print(response_text)\n",
    "\n",
    "    # Clean up potential markdown formatting (just in case)\n",
    "    if response_text.startswith(\"```\"):\n",
    "        # Remove markdown code blocks if present\n",
    "        response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # Attempt to parse the JSON\n",
    "    try:\n",
    "        event_data = json.loads(response_text)\n",
    "        print(\"\\nSuccessfully parsed JSON:\")\n",
    "        print(json.dumps(event_data, indent=2))\n",
    "        \n",
    "        # Validate the expected keys are present\n",
    "        required_keys = ['s1_photons', 's2_charge', 'recoil_energy_keV']\n",
    "        if all(key in event_data for key in required_keys):\n",
    "            print(\"\\n‚úÖ API connection and JSON output confirmed.\")\n",
    "            print(f\"   S1: {event_data['s1_photons']} photons\")\n",
    "            print(f\"   S2: {event_data['s2_charge']} electrons\")\n",
    "            print(f\"   Energy: {event_data['recoil_energy_keV']} keV\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Warning: Missing expected keys. Got: {list(event_data.keys())}\")\n",
    "            \n",
    "    except json.JSONDecodeError as je:\n",
    "        print(f\"\\n‚ö†Ô∏è JSON Parse Error: {je}\")\n",
    "        print(\"Response may contain extra text or formatting.\")\n",
    "\n",
    "except anthropic.APIError as api_err:\n",
    "    print(f\"‚ùå API Error: {api_err}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred: {e}\")\n",
    "    print(\"Please check your API key and environment setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/simulate.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_correlated_events(n_events, mean_s1, mean_s2, s1_std, s2_std, correlation):\n",
    "    \"\"\"\n",
    "    Generates correlated S1 and S2 events using a multivariate normal distribution.\n",
    "    \"\"\"\n",
    "    mean = [mean_s1, mean_s2]\n",
    "    cov_s1_s2 = correlation * s1_std * s2_std\n",
    "    cov_matrix = [[s1_std**2, cov_s1_s2], [cov_s1_s2, s2_std**2]]\n",
    "    \n",
    "    events = np.random.multivariate_normal(mean, cov_matrix, n_events)\n",
    "    events[events < 0] = 0  # Enforce physical constraint (no negative signals)\n",
    "    return events\n",
    "\n",
    "def generate_dataset(num_events=500, signal_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Generates a full dataset of mixed Nuclear Recoil (NR) and Electronic Recoil (ER) events.\n",
    "    \"\"\"\n",
    "    n_signal = int(num_events * signal_fraction)\n",
    "    n_background = num_events - n_signal\n",
    "\n",
    "    events_list = []\n",
    "\n",
    "    # --- Generate Nuclear Recoil (NR) events ---\n",
    "    nr_energies = np.random.exponential(scale=5.0, size=n_signal) + 2.0\n",
    "\n",
    "    for energy in nr_energies:\n",
    "        mean_s1 = energy * 2.0\n",
    "        mean_s2 = energy * 50.0\n",
    "        s1_std = np.sqrt(mean_s1) * 0.5\n",
    "        s2_std = np.sqrt(mean_s2) * 0.5\n",
    "        correlation = -0.6\n",
    "\n",
    "        s1, s2 = generate_correlated_events(1, mean_s1, mean_s2, s1_std, s2_std, correlation)[0]\n",
    "        \n",
    "        events_list.append({\n",
    "            'true_label': 'Nuclear Recoil',\n",
    "            'recoil_energy_keV': energy,\n",
    "            's1_photons': s1,\n",
    "            's2_charge': s2\n",
    "        })\n",
    "\n",
    "    # --- Generate Electronic Recoil (ER) events ---\n",
    "    er_energies = np.random.uniform(low=2.0, high=50.0, size=n_background)\n",
    "\n",
    "    for energy in er_energies:\n",
    "        mean_s1 = energy * 1.5\n",
    "        mean_s2 = energy * 200.0\n",
    "        s1_std = np.sqrt(mean_s1) * 0.7\n",
    "        s2_std = np.sqrt(mean_s2) * 0.7\n",
    "        correlation = -0.4\n",
    "\n",
    "        s1, s2 = generate_correlated_events(1, mean_s1, mean_s2, s1_std, s2_std, correlation)[0]\n",
    "        \n",
    "        events_list.append({\n",
    "            'true_label': 'Electronic Recoil',\n",
    "            'recoil_energy_keV': energy,\n",
    "            's1_photons': s1,\n",
    "            's2_charge': s2\n",
    "        })\n",
    "\n",
    "    # --- Finalize DataFrame ---\n",
    "    df = pd.DataFrame(events_list)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df['event_id'] = df.index\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    df['log10_s2_s1'] = np.log10((df['s2_charge'] + epsilon) / (df['s1_photons'] + epsilon))\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Generating simulated dark matter detector dataset...\")\n",
    "\n",
    "    output_dir = 'data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dataset = generate_dataset(num_events=500, signal_fraction=0.2)\n",
    "    output_path = os.path.join(output_dir, 'dataset.csv')\n",
    "    dataset.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Dataset with {len(dataset)} events saved to {output_path}\")\n",
    "    print(\"\\nDataset Head:\")\n",
    "    print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4609b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ‚úÖ Dummy classify_event (replace this later with your real Claude logic)\n",
    "def classify_event(row, client):\n",
    "    \"\"\"\n",
    "    Dummy event classification using Anthropic API.\n",
    "    Replace this with your actual prompt logic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Example call (commented out until you have Claude access)\n",
    "        # response = client.messages.create(\n",
    "        #     model=\"claude-3-sonnet-20240229\",\n",
    "        #     max_tokens=50,\n",
    "        #     messages=[{\"role\": \"user\", \"content\": f\"Classify this event: {row}\"}]\n",
    "        # )\n",
    "        # classification = response.content[0].text.strip()\n",
    "\n",
    "        classification = \"Sample_Category\"  # ‚Üê Placeholder for now\n",
    "        return {'event_id': row['event_id'], 'classification': classification}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API error for event {row.get('event_id', 'unknown')}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, classify all events, and save the results.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Step 1: Build a safe absolute path for the dataset\n",
    "    input_path = r\"C:\\Users\\wrich\\Downloads\\CluadeMain\\CODEFATHER_ClaudeSolvathon-1\\data\\dataset.csv\"\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå Error: Input file not found at {input_path}\")\n",
    "        print(\"Please run src/simulate.py first to generate the dataset.\")\n",
    "        return\n",
    "\n",
    "    # ‚úÖ Step 2: Load dataset\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "        print(f\"‚úÖ Loaded dataset successfully: {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # ‚úÖ Step 3: Initialize Anthropic client\n",
    "    try:\n",
    "        client = anthropic.Anthropic()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Anthropic client: {e}\")\n",
    "        print(\"Please ensure ANTHROPIC_API_KEY is set correctly.\")\n",
    "        return\n",
    "\n",
    "    # ‚úÖ Step 4: Classify events\n",
    "    results = []\n",
    "    print(\"üöÄ Starting event classification...\")\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        try:\n",
    "            result = classify_event(row, client)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error classifying event {index}: {e}\")\n",
    "        time.sleep(0.5)  # avoid API rate limits\n",
    "\n",
    "    if not results:\n",
    "        print(\"‚ùå No events were successfully classified. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # ‚úÖ Step 5: Merge and save\n",
    "    results_df = pd.DataFrame(results)\n",
    "    df['event_id'] = df['event_id'].astype(int)\n",
    "    results_df['event_id'] = results_df['event_id'].astype(int)\n",
    "\n",
    "    output_path = os.path.join(os.path.dirname(input_path), \"classified_dataset.csv\")\n",
    "    classified_df = pd.merge(df, results_df, on='event_id', how='left')\n",
    "    classified_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Classification complete. Saved to {output_path}\")\n",
    "    print(\"\\nüìä Preview of classified data:\")\n",
    "    print(classified_df.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: visualize_classification.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def load_classified_data(file_path):\n",
    "    \"\"\"\n",
    "    Load classified dataset from CSV or JSON.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str or Path): Path to the dataset.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset with expected columns.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if file_path.suffix == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.suffix in [\".json\", \".jsonl\"]:\n",
    "        df = pd.read_json(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use CSV or JSON.\")\n",
    "    \n",
    "    # Compute log10(S2/S1) if not already present\n",
    "    if 'log10_s2_s1' not in df.columns:\n",
    "        df['log10_s2_s1'] = np.log10(df['s2_charge'] / df['s1_photons'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_discrimination_plot(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Generates the S2/S1 vs. S1 scatter plot (\"money plot\").\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The classified dataset with columns:\n",
    "            - s1_photons\n",
    "            - s2_charge\n",
    "            - log10_s2_s1\n",
    "            - classification (predicted label)\n",
    "            - true_label (ground truth)\n",
    "        save_path (str, optional): Path to save the figure.\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # CORRECTED: Updated the style name for compatibility with modern Matplotlib\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x='s1_photons',\n",
    "        y='log10_s2_s1',\n",
    "        hue='classification',\n",
    "        style='true_label',\n",
    "        palette='Set2',\n",
    "        alpha=0.7,\n",
    "        s=60,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title('AI Classification of Simulated Detector Events', fontsize=16)\n",
    "    ax.set_xlabel('S1 Signal (photons) [log scale]', fontsize=12)\n",
    "    ax.set_ylabel('log10(S2 / S1)', fontsize=12)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.legend(title='Classification / Ground Truth', fontsize=10)\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def synthesize_summary(df):\n",
    "    \"\"\"\n",
    "    Generates a text summary of classification results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable summary.\n",
    "    \"\"\"\n",
    "    total_events = len(df)\n",
    "    correct = (df['classification'] == df['true_label']).sum()\n",
    "    accuracy = correct / total_events * 100\n",
    "    \n",
    "    nr_count = (df['true_label'] == 'NR').sum()\n",
    "    er_count = (df['true_label'] == 'ER').sum()\n",
    "    \n",
    "    summary = (\n",
    "        f\"Dataset contains {total_events} events:\\n\"\n",
    "        f\"  - NR events: {nr_count}\\n\"\n",
    "        f\"  - ER events: {er_count}\\n\"\n",
    "        f\"AI classifier correctly labeled {correct} events ({accuracy:.2f}% accuracy).\\n\"\n",
    "        f\"The 'money plot' shows the separation between NR and ER events in log10(S2/S1) vs S1 space.\\n\"\n",
    "        f\"Well-separated clusters indicate good discrimination performance.\"\n",
    "    )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # === 1. Load data ===\n",
    "    # Make sure to use the correct path for your system\n",
    "    data_file = \"C:/Users/wrich/Downloads/CluadeMain/CODEFATHER_ClaudeSolvathon-1/data/classified_dataset.csv\"\n",
    "    df = load_classified_data(data_file)\n",
    "    \n",
    "    # === 2. Generate plot ===\n",
    "    fig = create_discrimination_plot(df, save_path=\"money_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # === 3. Generate summary ===\n",
    "    summary_text = synthesize_summary(df)\n",
    "    print(\"\\n=== Classification Summary ===\\n\")\n",
    "    print(summary_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ec4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 01:17:00.849 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.850 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.850 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.851 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.852 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.852 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.853 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-11 01:17:00.853 No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LayoutsMixin.columns() missing 1 required positional argument: 'spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m df = load_data(DATA_FILE)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# --- Main Layout ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     col1, col2 = \u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m col1:\n\u001b[32m     58\u001b[39m         st.header(\u001b[33m\"\u001b[39m\u001b[33mEvent Classification Map\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\streamlit\\runtime\\metrics_util.py:443\u001b[39m, in \u001b[36mgather_metrics.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m         _LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[33mFailed to collect command telemetry\u001b[39m\u001b[33m\"\u001b[39m, exc_info=ex)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     result = \u001b[43mnon_optional_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RerunException:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Duplicated from below, because static analysis tools get confused\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# by deferring the rethrow.\u001b[39;00m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tracking_activated \u001b[38;5;129;01mand\u001b[39;00m command_telemetry:\n",
      "\u001b[31mTypeError\u001b[39m: LayoutsMixin.columns() missing 1 required positional argument: 'spec'"
     ]
    }
   ],
   "source": [
    "# dashboards/app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- Plotting Function (can be in a separate utils file) ---\n",
    "def create_discrimination_plot(df):\n",
    "    \"\"\"Generates the S2/S1 vs. S1 scatter plot.\"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x='s1_photons',\n",
    "        y='log10_s2_s1',\n",
    "        hue='classification',\n",
    "        style='true_label',\n",
    "        ax=ax,\n",
    "        alpha=0.8,\n",
    "        s=60\n",
    "    )\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title('AI Classification of Simulated Detector Events', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('S1 Signal (photons) [log scale]', fontsize=12)\n",
    "    ax.set_ylabel('log10(S2 / S1)', fontsize=12)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.legend(title='Legend', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# --- Streamlit App Main Logic ---\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "st.title(\"Dark Matter Scribe\")\n",
    "st.markdown(\"An AI system for classifying and reasoning about dark matter detector events.\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_data(filepath):\n",
    "    \"\"\"Cached function to load the dataset.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        return pd.read_csv(filepath)\n",
    "    return None\n",
    "\n",
    "# Load the classified data\n",
    "DATA_FILE = 'C:\\Users\\wrich\\Downloads\\CluadeMain\\CODEFATHER_ClaudeSolvathon-1\\data\\classified_dataset.csv'\n",
    "df = load_data(DATA_FILE)\n",
    "\n",
    "if df is not None:\n",
    "    # --- Main Layout ---\n",
    "    col1, col2 = st.columns()\n",
    "\n",
    "    with col1:\n",
    "        st.header(\"Event Classification Map\")\n",
    "        st.pyplot(create_discrimination_plot(df))\n",
    "\n",
    "    with col2:\n",
    "        st.header(\"Event Inspector\")\n",
    "        \n",
    "        # Dropdown to select an event\n",
    "        event_ids = df['event_id'].tolist()\n",
    "        selected_id = st.selectbox(\"Select an Event ID to inspect:\", event_ids)\n",
    "        \n",
    "        if selected_id is not None:\n",
    "            # Get the data for the selected event\n",
    "            event_details = df[df['event_id'] == selected_id].iloc\n",
    "            \n",
    "            st.subheader(f\"Details for Event ID: {selected_id}\")\n",
    "            \n",
    "            # Display key features\n",
    "            st.metric(\"True Label\", event_details['true_label'])\n",
    "            st.metric(\"AI Classification\", event_details['classification'])\n",
    "            st.metric(\"AI Confidence\", f\"{event_details['confidence']:.2f}\")\n",
    "\n",
    "            st.markdown(\"---\")\n",
    "            \n",
    "            # Display the AI's reasoning\n",
    "            st.subheader(\"AI Physicist's Reasoning\")\n",
    "            st.info(event_details['reasoning'])\n",
    "\n",
    "else:\n",
    "    st.error(f\"Could not find the dataset at '{DATA_FILE}'. Please run the simulation and classification scripts first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
